{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr0isXXkfYArUt9P5JlNyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDaniyalTariq/Urdu_Handwritten_Words_Recognition_through_CNN_Ensembling/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to load and preprocess data from a specific JSON file (for each character)\n",
        "def load_and_preprocess_data(dataset_folder):\n",
        "    data = []\n",
        "    labels = []\n",
        "    character_labels = {}  # A dictionary to map character to a unique label\n",
        "\n",
        "    # Get all JSON files in the folder\n",
        "    files = os.listdir(dataset_folder)\n",
        "    for idx, file_name in enumerate(files):\n",
        "        if file_name.endswith('.json'):\n",
        "            label = idx  # Unique label based on the order of the files\n",
        "            character_labels[file_name] = label\n",
        "            file_path = os.path.join(dataset_folder, file_name)\n",
        "            try:\n",
        "                with open(file_path, 'r') as file:\n",
        "                    json_data = json.load(file)\n",
        "                    features = []\n",
        "                    # Flatten each stroke into a sequence of points\n",
        "                    for stroke in json_data:\n",
        "                        for point in stroke:\n",
        "                            features.append([point['dx'], point['dy'], point['timestamp']])\n",
        "                    data.append(features)\n",
        "                    labels.append(label)  # Add the unique label for this character\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error reading JSON file {file_path}: {e}\")\n",
        "\n",
        "    # Normalize the features (dx, dy, timestamp)\n",
        "    scaler = StandardScaler()\n",
        "    data = [scaler.fit_transform(np.array(seq)[:, :3]) for seq in data]  # Normalize dx, dy, timestamp\n",
        "\n",
        "    # Pad sequences to ensure uniform length\n",
        "    max_sequence_length = max(len(seq) for seq in data)\n",
        "    padded_data = pad_sequences(data, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "    return np.array(padded_data), np.array(labels), character_labels\n",
        "\n",
        "# Define a simple LSTM model for recognition\n",
        "def create_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, activation='relu', input_shape=input_shape, return_sequences=True))\n",
        "    model.add(LSTM(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))  # Multi-class classification\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Function to train the model for all characters\n",
        "def train_model_on_dataset(dataset_folder):\n",
        "    if not os.path.exists(dataset_folder):\n",
        "        raise FileNotFoundError(f\"Dataset folder '{dataset_folder}' does not exist.\")\n",
        "\n",
        "    # Load and preprocess data\n",
        "    X, y, character_labels = load_and_preprocess_data(dataset_folder)\n",
        "\n",
        "    # Create the model\n",
        "    model = create_model(X.shape[1:], len(character_labels))  # len(character_labels) is the number of unique characters\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save('urdu_character_model.h5')\n",
        "\n",
        "    return model, character_labels\n",
        "\n",
        "# Prediction function for user input (drawing)\n",
        "def predict_character(user_input_strokes, model, character_labels):\n",
        "    # Preprocess the user's input similar to how we preprocess training data\n",
        "    scaler = StandardScaler()\n",
        "    user_input = [scaler.fit_transform(np.array(user_input_strokes)[:, :3])]\n",
        "    max_sequence_length = max(len(seq) for seq in user_input)\n",
        "    padded_input = pad_sequences(user_input, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "    # Predict the character\n",
        "    prediction = model.predict(padded_input)\n",
        "\n",
        "    # Get the predicted class (the index with the highest probability)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Get the corresponding character from the labels\n",
        "    predicted_character = list(character_labels.keys())[predicted_class]\n",
        "\n",
        "    return predicted_character\n",
        "\n",
        "# Define the path to your dataset folder\n",
        "dataset_folder = '/content/dataset/json'\n",
        "\n",
        "# Train the model\n",
        "model, character_labels = train_model_on_dataset(dataset_folder)\n",
        "\n",
        "# Example of how to predict a user input (you need to simulate user input here):\n",
        "user_input_strokes = [\n",
        "    {'dx': 224, 'dy': 119, 'timestamp': 0},\n",
        "    {'dx': 224.33, 'dy': 119.66, 'timestamp': 0},\n",
        "    {'dx': 224.66, 'dy': 121.33, 'timestamp': 0},\n",
        "    {'dx': 225.33, 'dy': 124, 'timestamp': 17},\n",
        "    {'dx': 225.66, 'dy': 127, 'timestamp': 17},\n",
        "    # Add more points to simulate user's stroke data\n",
        "]\n",
        "\n",
        "# Predict the character based on the user's input\n",
        "predicted_character = predict_character(user_input_strokes, model, character_labels)\n",
        "print(f\"Predicted Character: {predicted_character}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "5cs0CRlgYw5x",
        "outputId": "a562b593-6b54-41fa-dd78-453014afcd9d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 125s/step - accuracy: 0.0000e+00 - loss: 5.5841 - val_accuracy: 0.0000e+00 - val_loss: 5.5876\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 124s/step - accuracy: 0.0000e+00 - loss: 5.5823 - val_accuracy: 0.0000e+00 - val_loss: 5.5935\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m932s\u001b[0m 127s/step - accuracy: 0.0068 - loss: 5.5809 - val_accuracy: 0.0000e+00 - val_loss: 5.5995\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 133s/step - accuracy: 0.0000e+00 - loss: 5.5793 - val_accuracy: 0.0000e+00 - val_loss: 5.6054\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m917s\u001b[0m 124s/step - accuracy: 0.0000e+00 - loss: 5.5778 - val_accuracy: 0.0000e+00 - val_loss: 5.6114\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m962s\u001b[0m 130s/step - accuracy: 0.0000e+00 - loss: 5.5763 - val_accuracy: 0.0000e+00 - val_loss: 5.6174\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m900s\u001b[0m 127s/step - accuracy: 0.0026 - loss: 5.5747 - val_accuracy: 0.0000e+00 - val_loss: 5.6234\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 130s/step - accuracy: 0.0000e+00 - loss: 5.5734 - val_accuracy: 0.0000e+00 - val_loss: 5.6294\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m893s\u001b[0m 128s/step - accuracy: 0.0049 - loss: 5.5720 - val_accuracy: 0.0000e+00 - val_loss: 5.6354\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m898s\u001b[0m 122s/step - accuracy: 0.0012 - loss: 5.5703 - val_accuracy: 0.0000e+00 - val_loss: 5.6413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a478db367039>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Predict the character based on the user's input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mpredicted_character\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_character\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input_strokes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted Character: {predicted_character}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a478db367039>\u001b[0m in \u001b[0;36mpredict_character\u001b[0;34m(user_input_strokes, model, character_labels)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Preprocess the user's input similar to how we preprocess training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input_strokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mmax_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    }
  ]
}